# streaming_data_processing

---

## Used Technologies and Services

---

- Item 1 Apache Airflow
- Item 2 Apache Zookeeper
- Item 3 Apache Kafka
- Item 4 Apache Hadoop HDFS
- Item 5 Apache Spark (PySpark)
- Item 6 Hadoop YARN
- Item 7 Elasticsearch
- Item 8 Kibana
- Item 9 MinIO
- Item 10 Docker
- Item 11 OS: Linux 
- Item 12: PyCharm, VSCode

## Overview

- Item 1 Download compressed data source from a URL
- Item 2 processing the downloaded raw data with PySpark, and use HDFS as file storage, check resources with Apache Hadoop YARN.
- Item 3 Use data-generator to simulate streaming data, and send the data to Apache Kafka.
- Item 4 Read the streaming data to Elasticsearchm and visualize it using kibana.
- Item 5 Write the streaming data to MinIO (AWS object Storage) 
- Item 6 Use Apache Airflow to orchestrate the whole data pipeline.

![logo](https://github.com/raisiali2/streaming_data_processing/blob/setup/core-infrastructure/img/plan.jpeg?raw=true)

---

